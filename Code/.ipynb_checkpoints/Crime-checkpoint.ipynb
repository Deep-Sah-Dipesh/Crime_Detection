{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e59589e3-5336-4ea8-97a2-7fc1e774ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai-whisper in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (20240930)\n",
      "Requirement already satisfied: ffmpeg-python in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.2.0)\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\lib\\site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from openai-whisper) (2.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai-whisper) (4.66.5)\n",
      "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from openai-whisper) (10.3.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from openai-whisper) (0.9.0)\n",
      "Requirement already satisfied: future in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch->openai-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->openai-whisper) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d970f7-1f50-46ea-a821-eea40fa5dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Audio extraction failed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths using raw strings (r\"...\") to avoid escape sequence issues\n",
    "video_path = r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\Videos\\1-1004\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.mp4\"\n",
    "audio_path = r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\Audios\\1-1004\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.wav\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
    "\n",
    "# Correct FFmpeg command\n",
    "ffmpeg_command = f'ffmpeg -i \"{video_path}\" -q:a 0 -map a \"{audio_path}\" -y'\n",
    "os.system(ffmpeg_command)\n",
    "\n",
    "# Verify if audio extraction was successful\n",
    "if os.path.exists(audio_path):\n",
    "    print(\"Audio extraction successful:\", audio_path)\n",
    "else:\n",
    "    print(\"Error: Audio extraction failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b277e9-5bb0-4109-ae1c-506021e137fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59db009-cbe5-4e32-b34b-be0b1604d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d958932-c53c-41ae-96fb-dbc6b6024784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ffmpeg in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36debd6b-e97b-48ef-8685-5807623ff37c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Extract audio from video\u001b[39;00m\n\u001b[0;32m     15\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m video_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m\"\u001b[39m, video_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-q:a\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, audio_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-y\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Transcribe the extracted audio\u001b[39;00m\n\u001b[0;32m     19\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(audio_path)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import subprocess\n",
    "\n",
    "# Load the model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Set your video path (update to the correct location)\n",
    "video_path = r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\Videos\\1-1004\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.mp4\"\n",
    "\n",
    "# Extract the video filename (without extension)\n",
    "video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "# Extract audio from video\n",
    "audio_path = video_path.replace(\".mp4\", \".mp3\")\n",
    "subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\", audio_path, \"-y\"])\n",
    "\n",
    "# Transcribe the extracted audio\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# Print the transcribed text\n",
    "print(\"Transcription:\\n\", result[\"text\"])\n",
    "\n",
    "# Set transcript save location\n",
    "transcript_folder = r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\Transcripts\"\n",
    "os.makedirs(transcript_folder, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "transcript_path = os.path.join(transcript_folder, f\"{video_name}.txt\")\n",
    "\n",
    "# Write the transcript to a file\n",
    "with open(transcript_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(result[\"text\"])\n",
    "\n",
    "print(f\"âœ… Transcript saved at: {transcript_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b492298-2660-4dcd-aa6f-28cb037f3148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorchvideo in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.1.5)\n",
      "Requirement already satisfied: torch in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.22.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: fvcore in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from pytorchvideo) (0.1.5.post20221221)\n",
      "Requirement already satisfied: av in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from pytorchvideo) (14.3.0)\n",
      "Requirement already satisfied: parameterized in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: iopath in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from pytorchvideo) (0.1.10)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorchvideo) (3.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fvcore->pytorchvideo) (6.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from fvcore->pytorchvideo) (4.66.5)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from fvcore->pytorchvideo) (3.1.0)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from fvcore->pytorchvideo) (0.9.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from iopath->pytorchvideo) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\programdata\\anaconda3\\lib\\site-packages (from portalocker->iopath->pytorchvideo) (305.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->fvcore->pytorchvideo) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorchvideo torch torchvision opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3159f62-f852-41de-9010-88e58fb6448a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     72\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrosyd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCrime_Detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1-1004\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mA.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 73\u001b[0m extract_video_features(video_path, model)\n",
      "Cell \u001b[1;32mIn[15], line 53\u001b[0m, in \u001b[0;36mextract_video_features\u001b[1;34m(video_path, model, base_save_path)\u001b[0m\n\u001b[0;32m     50\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_save_path, video_name)  \u001b[38;5;66;03m# Create a folder for each video\u001b[39;00m\n\u001b[0;32m     51\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Ensure directory exists\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m video_clips \u001b[38;5;241m=\u001b[39m preprocess_video(video_path)  \u001b[38;5;66;03m# Preprocess video into 16-frame clips\u001b[39;00m\n\u001b[0;32m     54\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[15], line 34\u001b[0m, in \u001b[0;36mpreprocess_video\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[0;32m     33\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m---> 34\u001b[0m frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(frames)  \u001b[38;5;66;03m# Shape: (num_frames, 3, 224, 224)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Split into clips of 16 frames each\u001b[39;00m\n\u001b[0;32m     37\u001b[0m clips \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pytorchvideo.models.hub import i3d_r50\n",
    "\n",
    "# Load the I3D model pre-trained on the Kinetics dataset\n",
    "model = i3d_r50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "def preprocess_video(video_path):\n",
    "    \"\"\"\n",
    "    Preprocess the input video to extract clips of shape (3, 16, 224, 224)\n",
    "    suitable for I3D input.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = transform(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    frames = torch.stack(frames)  # Shape: (num_frames, 3, 224, 224)\n",
    "\n",
    "    # Split into clips of 16 frames each\n",
    "    clips = []\n",
    "    for i in range(0, len(frames) - 15, 16):  # Sliding window of 16 frames\n",
    "        clip = frames[i:i+16]\n",
    "        clips.append(clip)\n",
    "\n",
    "    return clips  # List of (16, 3, 224, 224) tensors\n",
    "\n",
    "def extract_video_features(video_path, model, base_save_path=r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\video_features\"):\n",
    "    \"\"\"\n",
    "    Extract features from a given video and save them in a folder named\n",
    "    after the full video name.\n",
    "    \"\"\"\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]  # Preserve full filename\n",
    "    save_path = os.path.join(base_save_path, video_name)  # Create a folder for each video\n",
    "    os.makedirs(save_path, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    video_clips = preprocess_video(video_path)  # Preprocess video into 16-frame clips\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, clip in enumerate(video_clips):\n",
    "            clip = clip.unsqueeze(0)  # Add batch dimension (1, 16, 3, 224, 224)\n",
    "            clip = clip.permute(0, 2, 1, 3, 4)  # Ensure correct shape (1, 3, 16, 224, 224)\n",
    "\n",
    "            feature = model(clip)  # Extract features\n",
    "            feature = feature.squeeze().detach().cpu().numpy()  # Convert to NumPy\n",
    "\n",
    "            features.append(feature)\n",
    "\n",
    "            # Save each extracted feature as a NumPy file\n",
    "            np.save(os.path.join(save_path, f\"feature_{i}.npy\"), feature)\n",
    "\n",
    "    print(f\"Features saved successfully in: {save_path}\")\n",
    "\n",
    "# Example usage:\n",
    "video_path = r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\1-1004\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.mp4\"\n",
    "extract_video_features(video_path, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c3dee0-9eb3-48ae-94dd-04e16e28f625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-hub in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.16.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-hub) (4.25.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c102b2ca-b184-4461-a15f-f2923854bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16412b3a-6614-473b-8515-bad95b3db774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow-hub in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982fad8-df0f-4be0-b876-65e2f7583662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "# Load YAMNet model\n",
    "def load_yamnet_model():\n",
    "    model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "    return model\n",
    "\n",
    "# Function to extract audio features using YAMNet\n",
    "def extract_audio_features(audio_path, model, base_save_path=r\"C:\\Users\\rosyd\\Crime_Detection\\Training\\audio_features\"):\n",
    "    # Use the full audio file name as is\n",
    "    audio_name = os.path.basename(audio_path)  # Keep the full name (including extension)\n",
    "    save_path = os.path.join(base_save_path, audio_name)\n",
    "    os.makedirs(save_path, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "    # Load audio file\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000, mono=True)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "\n",
    "    # Convert to TensorFlow tensor\n",
    "    waveform_tensor = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "\n",
    "    # Run YAMNet model\n",
    "    scores, embeddings, spectrogram = model(waveform_tensor)  # Extract features\n",
    "    embeddings_np = embeddings.numpy()  # Convert tensor to NumPy array\n",
    "\n",
    "    # Save extracted features\n",
    "    np.save(os.path.join(save_path, f\"{audio_name}_features.npy\"), embeddings_np)\n",
    "    print(f\"Audio features saved successfully in: {os.path.join(save_path, f'{audio_name}_features.npy')}\")\n",
    "\n",
    "# Load YAMNet model\n",
    "yamnet_model = load_yamnet_model()\n",
    "\n",
    "# Example usage (update with your actual audio file path)\n",
    "audio_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Training\\audio\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.wav\"\n",
    "extract_audio_features(audio_path, yamnet_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6912c198-43f1-4e76-a1b7-8797879bcb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (2.7.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torchvision-0.22.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.6/1.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 2.1/2.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.7.0 torchvision-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae9d052c-9a27-43b4-a78d-b378cb13f2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.4 MB 1.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.6/10.4 MB 2.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.4/10.4 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/10.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.7/10.4 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.8/10.4 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.4/10.4 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.4 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.4/2.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 9.3 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\rosyd\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\rosyd\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea956b3e-9eec-45ef-be36-7061c13e0732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features saved successfully in: C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Training\\text_features\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.txt\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.txt_features.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the RoBERTa model and tokenizer\n",
    "def load_roberta_model():\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaModel.from_pretrained('roberta-base')\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to extract text features from the transcript\n",
    "def extract_text_features(transcript_path, tokenizer, model, base_save_path=r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Training\\text_features\"):\n",
    "    # Load the transcript\n",
    "    with open(transcript_path, 'r', encoding='utf-8') as file:\n",
    "        transcript = file.read()\n",
    "\n",
    "    # Tokenize the transcript and convert to tensor\n",
    "    inputs = tokenizer(transcript, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Get the RoBERTa embeddings (last hidden state)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # The embeddings are in outputs.last_hidden_state\n",
    "    embeddings = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "    # You can take the [CLS] token (first token) representation as the feature vector\n",
    "    cls_embedding = embeddings[:, 0, :].cpu().numpy()  # (batch_size, hidden_size)\n",
    "\n",
    "    # Save the extracted features using the complete transcript name\n",
    "    transcript_name = os.path.basename(transcript_path)  # Full name with extension\n",
    "    save_path = os.path.join(base_save_path, transcript_name)  # Save in a folder with the full name\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    # Save as a NumPy array\n",
    "    feature_file = os.path.join(save_path, f\"{transcript_name}_features.npy\")\n",
    "    np.save(feature_file, cls_embedding)\n",
    "    print(f\"Text features saved successfully in: {feature_file}\")\n",
    "\n",
    "# Load RoBERTa model\n",
    "tokenizer, model = load_roberta_model()\n",
    "\n",
    "# Example usage (update with your actual transcript file path)\n",
    "transcript_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Training\\transcripts\\A.Beautiful.Mind.2001__#00-01-45_00-02-50_label_A.txt\"\n",
    "extract_text_features(transcript_path, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75bbc34c-57ee-46e2-8f2e-8fd94ebbd784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Paths to feature folders\n",
    "video_feature_path = \"video_features/\"\n",
    "audio_feature_path = \"audio_features/\"\n",
    "text_feature_path = \"text_features/\"\n",
    "annotations_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Test\\annotations.txt\"\n",
    "\n",
    "# Load annotations\n",
    "def load_annotations(annotations_file):\n",
    "    annotations = {}\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            video_name = parts[0]  # First column is video name\n",
    "            label = int(parts[1])  # Second column is label\n",
    "            annotations[video_name] = label\n",
    "    return annotations\n",
    "\n",
    "annotations = load_annotations(annotations_path)\n",
    "\n",
    "# Custom Dataset for Meta-Learning\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, video_path, audio_path, text_path, annotations):\n",
    "        self.video_path = video_path\n",
    "        self.audio_path = audio_path\n",
    "        self.text_path = text_path\n",
    "        self.annotations = annotations\n",
    "        self.video_list = list(annotations.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.video_list[idx]\n",
    "\n",
    "        # Load features\n",
    "        video_feat = np.load(os.path.join(self.video_path, f\"{video_name}.npy\"))\n",
    "        audio_feat = np.load(os.path.join(self.audio_path, f\"{video_name}.npy\"))\n",
    "        text_feat = np.load(os.path.join(self.text_path, f\"{video_name}.npy\"))\n",
    "\n",
    "        # Concatenate features\n",
    "        features = np.concatenate([video_feat.flatten(), audio_feat.flatten(), text_feat.flatten()])\n",
    "\n",
    "        # Get label\n",
    "        label = self.annotations[video_name]\n",
    "\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Load dataset\n",
    "dataset = CrimeDataset(video_feature_path, audio_feature_path, text_feature_path, annotations)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a55b857a-9f8f-486b-bcd8-8c60d7b4e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 videos with labels.\n",
      "Unique crime classes: {'B4', '0', 'B5', 'B6', 'B2', 'G', 'B1'}\n",
      "Example mapping: {'0': 0, 'B1': 1, 'B2': 2, 'B4': 3, 'B5': 4, 'B6': 5, 'G': 6}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Path to annotations file\n",
    "annotations_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Test\\annotations.txt\"\n",
    "\n",
    "# Function to load annotations\n",
    "def load_annotations(annotations_file):\n",
    "    annotations = {}  # Dictionary to store video -> label mapping\n",
    "    label_set = set()  # Collect unique labels\n",
    "\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            video_name = parts[0]  # First column is video name\n",
    "            \n",
    "            # Extract labels from filename using regex\n",
    "            match = re.search(r\"_label_([A-Z0-9\\-]+)\", video_name)\n",
    "            if match:\n",
    "                raw_labels = match.group(1)\n",
    "                labels = raw_labels.split(\"-\")  # Labels may be multiple (G-B2-B6)\n",
    "                \n",
    "                annotations[video_name] = labels\n",
    "                label_set.update(labels)  # Store all unique labels\n",
    "    \n",
    "    return annotations, label_set\n",
    "\n",
    "# Load annotations\n",
    "annotations, unique_labels = load_annotations(annotations_path)\n",
    "\n",
    "# Mapping labels to class indices\n",
    "label_to_idx = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "output_dim = len(label_to_idx)  # Number of crime classes\n",
    "\n",
    "# Print some results\n",
    "print(f\"Loaded {len(annotations)} videos with labels.\")\n",
    "print(f\"Unique crime classes: {unique_labels}\")\n",
    "print(f\"Example mapping: {label_to_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "995467a4-e8ff-42f5-9671-f1ec5fd3668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 videos with labels.\n",
      "Example: [('v=S-7rRLrxnVQ__#1_label_B4-0-0', ['B4', '0']), ('v=u5SF4SlqNDQ__#00-00-00_00-01-00_label_G-0-0', ['G', '0']), ('v=u5SF4SlqNDQ__#00-02-39_00-03-41_label_G-0-0', ['G', '0']), ('v=cEOM18n8fhU__#1_label_G-0-0', ['G', '0']), ('v=NnmqkS1e88s__#1_label_B4-0-0', ['B4', '0'])]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "annotations_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Test\\annotations.txt\"\n",
    "\n",
    "# Define a dictionary to store video labels\n",
    "video_labels = defaultdict(set)  # Use set to avoid duplicate labels\n",
    "\n",
    "# Read and parse annotations.txt\n",
    "with open(annotations_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()  # Split line by spaces\n",
    "        if len(parts) < 2:\n",
    "            continue  # Skip empty or malformed lines\n",
    "        \n",
    "        video_name = parts[0]  # First part is the video name\n",
    "        \n",
    "        # Extract label from the video name using regex\n",
    "        match = re.search(r\"label_([A-Z0-9\\-]+)\", video_name)\n",
    "        if match:\n",
    "            raw_labels = match.group(1).split(\"-\")  # Labels are separated by \"-\"\n",
    "            for label in raw_labels:\n",
    "                if label:  # Avoid empty strings\n",
    "                    video_labels[video_name].add(label)\n",
    "\n",
    "# Convert sets to lists for easier processing\n",
    "video_labels = {video: list(labels) for video, labels in video_labels.items()}\n",
    "\n",
    "# Print sample results\n",
    "print(f\"Loaded {len(video_labels)} videos with labels.\")\n",
    "print(\"Example:\", list(video_labels.items())[:5])  # Print 5 sample entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b6f10bb-7019-4445-9083-48610debd7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All labels match the expected categories.\n"
     ]
    }
   ],
   "source": [
    "# Expected labels\n",
    "expected_labels = {\"B1\", \"B2\", \"B4\", \"B5\", \"B6\", \"G\", \"0\"}\n",
    "\n",
    "# Find unexpected labels\n",
    "unexpected_labels = set()\n",
    "for labels in video_labels.values():\n",
    "    for label in labels:\n",
    "        if label not in expected_labels:\n",
    "            unexpected_labels.add(label)\n",
    "\n",
    "# Print results\n",
    "if unexpected_labels:\n",
    "    print(\"âš ï¸ Unexpected labels found:\", unexpected_labels)\n",
    "else:\n",
    "    print(\"âœ… All labels match the expected categories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd5d0d79-b7f0-402d-85ed-eb68aa6f69c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {'B4': 101, '0': 496, 'G': 103, 'B6': 106, 'B1': 126, 'B2': 104, 'B5': 11}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = Counter()\n",
    "for labels in video_labels.values():\n",
    "    label_counts.update(labels)\n",
    "\n",
    "# Print label frequencies\n",
    "print(\"Label distribution:\", dict(label_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7004fc-aae5-4ba3-b579-3af2f40e618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Count: 1\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())  # Should print True\n",
    "print(\"GPU Count:\", torch.cuda.device_count())  # Should print 1 (or more)\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))  # Should print \"NVIDIA GeForce RTX 4060\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bf54907-b7a0-4b86-a835-12d8f60a03ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_video_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mharsh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Now load the video labels\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m video_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_video_labels\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_video_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the correct working directory\n",
    "os.chdir(r\"C:\\Users\\harsh\\OneDrive\\Desktop\\dataset\\Test\")\n",
    "\n",
    "# Now load the video labels\n",
    "video_labels = load_video_labels(\"annotations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b09f97c-e97d-4e5d-b001-ea8d85c5b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1/5 Complete\n",
      "âœ… Epoch 2/5 Complete\n",
      "âœ… Epoch 3/5 Complete\n",
      "âœ… Epoch 4/5 Complete\n",
      "âœ… Epoch 5/5 Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ðŸ”¥ One-Hot Encoding for Labels\n",
    "def one_hot_encode(labels, label_to_index):\n",
    "    vector = np.zeros(len(label_to_index))\n",
    "    for label in labels:\n",
    "        if label in label_to_index:\n",
    "            vector[label_to_index[label]] = 1\n",
    "    return vector\n",
    "\n",
    "# Define label mapping\n",
    "label_to_index = {'0': 0, 'B1': 1, 'B2': 2, 'B4': 3, 'B5': 4, 'B6': 5, 'G': 6}\n",
    "num_classes = len(label_to_index)\n",
    "\n",
    "# ðŸ”¥ Load Features for Each Modality\n",
    "def load_features(video_name):\n",
    "    video_path = f\"video_features/{video_name}.npy\"\n",
    "    audio_path = f\"audio_features/{video_name}.npy\"\n",
    "    text_path = f\"text_features/{video_name}.npy\"\n",
    "    \n",
    "    if not os.path.exists(video_path) or not os.path.exists(audio_path) or not os.path.exists(text_path):\n",
    "        return None\n",
    "\n",
    "    video_feat = np.load(video_path)\n",
    "    audio_feat = np.load(audio_path)\n",
    "    text_feat = np.load(text_path)\n",
    "\n",
    "    return torch.tensor(video_feat, dtype=torch.float32), torch.tensor(audio_feat, dtype=torch.float32), torch.tensor(text_feat, dtype=torch.float32)\n",
    "\n",
    "# ðŸ”¥ Load Labels from File\n",
    "def load_video_labels(annotation_file=\"annotations.txt\"):\n",
    "    video_labels = {}\n",
    "    with open(annotation_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            video_name = parts[0]  # First column is the video name\n",
    "            labels = parts[1:]  # Remaining columns are labels\n",
    "            video_labels[video_name] = labels\n",
    "    return video_labels\n",
    "\n",
    "# ðŸ”¥ Dataset Class for Meta-Learning\n",
    "class CrimeMetaDataset(Dataset):\n",
    "    def __init__(self, video_labels, k_shot=5, query_size=3):\n",
    "        self.video_labels = video_labels\n",
    "        self.video_names = list(video_labels.keys())\n",
    "        self.k_shot = k_shot\n",
    "        self.query_size = query_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_labels) // (self.k_shot + self.query_size)\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        sampled_videos = random.sample(self.video_names, self.k_shot + self.query_size)\n",
    "        support_videos = sampled_videos[:self.k_shot]\n",
    "        query_videos = sampled_videos[self.k_shot:]\n",
    "        \n",
    "        support_set, query_set = [], []\n",
    "        \n",
    "        for video in support_videos:\n",
    "            features = load_features(video)\n",
    "            if features:\n",
    "                label = one_hot_encode(self.video_labels[video], label_to_index)\n",
    "                support_set.append((*features, torch.tensor(label, dtype=torch.float32)))\n",
    "        \n",
    "        for video in query_videos:\n",
    "            features = load_features(video)\n",
    "            if features:\n",
    "                label = one_hot_encode(self.video_labels[video], label_to_index)\n",
    "                query_set.append((*features, torch.tensor(label, dtype=torch.float32)))\n",
    "        \n",
    "        return support_set, query_set\n",
    "\n",
    "# ðŸ”¥ Adaptive Weighting Mechanism\n",
    "class AdaptiveWeighting(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AdaptiveWeighting, self).__init__()\n",
    "        self.attention = nn.Linear(input_dim, 3)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, video_feat, audio_feat, text_feat):\n",
    "        combined = torch.cat([video_feat, audio_feat, text_feat], dim=-1)\n",
    "        weights = self.softmax(self.attention(combined))\n",
    "        weighted_features = (weights[:, 0].unsqueeze(1) * video_feat +\n",
    "                             weights[:, 1].unsqueeze(1) * audio_feat +\n",
    "                             weights[:, 2].unsqueeze(1) * text_feat)\n",
    "        return weighted_features\n",
    "\n",
    "# ðŸ”¥ Meta-Learner Model\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.adaptive_weighting = AdaptiveWeighting(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, video_x, audio_x, text_x):\n",
    "        fused_features = self.adaptive_weighting(video_x, audio_x, text_x)\n",
    "        x = self.relu(self.fc1(fused_features))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ðŸ”¥ MAML Implementation\n",
    "class MAML:\n",
    "    def __init__(self, model, lr_inner=0.01, lr_outer=0.001, k_shot=5, meta_batch_size=4):\n",
    "        self.model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.k_shot = k_shot\n",
    "        self.meta_batch_size = meta_batch_size\n",
    "        self.inner_lr = lr_inner\n",
    "        self.outer_lr = lr_outer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.outer_lr)\n",
    "\n",
    "    def inner_update(self, support_set):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        inner_optimizer = optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "        loss = None\n",
    "        has_valid_sample = False\n",
    "        \n",
    "        for sample in support_set:\n",
    "            if len(sample) != 4:\n",
    "                continue\n",
    "            \n",
    "            video_x, audio_x, text_x, y_s = sample\n",
    "            if video_x is None or audio_x is None or text_x is None:\n",
    "                continue\n",
    "            \n",
    "            has_valid_sample = True\n",
    "            video_x, audio_x, text_x, y_s = video_x.to(\"cuda\"), audio_x.to(\"cuda\"), text_x.to(\"cuda\"), y_s.to(\"cuda\")\n",
    "            preds = self.model(video_x, audio_x, text_x)\n",
    "            loss = criterion(preds, y_s)\n",
    "            \n",
    "            inner_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        \n",
    "        return loss.item() if has_valid_sample else 0.0\n",
    "    \n",
    "    def outer_update(self, query_set):\n",
    "        meta_loss = torch.tensor(0.0, requires_grad=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for video_x, audio_x, text_x, y_q in query_set:\n",
    "            video_x, audio_x, text_x, y_q = (\n",
    "                video_x.to(\"cuda\"),\n",
    "                audio_x.to(\"cuda\"),\n",
    "                text_x.to(\"cuda\"),\n",
    "                y_q.to(\"cuda\"),\n",
    "            )\n",
    "            preds = self.model(video_x, audio_x, text_x)\n",
    "            loss = criterion(preds, y_q)\n",
    "            meta_loss += loss  # Keep accumulating loss as a tensor\n",
    "\n",
    "        if meta_loss.item() == 0.0:  # No valid samples, avoid calling backward()\n",
    "            return 0.0\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return meta_loss.item()\n",
    "\n",
    "\n",
    "# ðŸ”¥ Training Loop\n",
    "def train_maml(video_labels, epochs=5):\n",
    "    dataset = CrimeMetaDataset(video_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    input_dim = 512 + 1024 + 768\n",
    "    maml = MAML(MetaLearner(input_dim, hidden_dim=256, output_dim=num_classes))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for support_set, query_set in dataloader:\n",
    "            maml.inner_update(support_set)\n",
    "            maml.outer_update(query_set)\n",
    "        print(f\"âœ… Epoch {epoch+1}/{epochs} Complete\")\n",
    "    \n",
    "    return maml.model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_labels = load_video_labels(\"annotations.txt\")\n",
    "    trained_model = train_maml(video_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1b82-8f53-4663-95fc-d9feb95dd42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
